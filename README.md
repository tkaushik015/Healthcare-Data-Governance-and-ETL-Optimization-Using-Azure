# ğŸš€ **Healthcare Revenue Cycle Management (RCM) Data Engineering Project using Azure**  
ğŸ”— [**GitHub Repository**](https://github.com/tkaushik015/Healthcare-Data-Governance-and-ETL-Optimization-Using-Azure)

---

## ğŸ“š **Project Overview**  
An **end-to-end data engineering pipeline** designed for the **healthcare RCM domain**, focusing on **Accounts Receivable (AR)** processes to ensure **financial health** of hospitals. Implemented using **Azure Data Engineering Stack** with real-world scenarios like multi-source data integration, schema merging, and KPI generation for hospital revenue tracking.

---

## ğŸ’¾ **Project Architecture**  
ğŸ“Š **Architecture Flow:**

```
          +----------------+          +----------------+          +---------------+
          |  Azure SQL DB  |          |  ADLS Gen2     |          |   Public APIs  |
          | (EMR Data)     |          | (Claims Data)  |          | (NPI, ICD)     |
          +--------+-------+          +--------+-------+          +-------+-------+
                   |                           |                          |
                   +---------------------------+--------------------------+
                                                   |
                                        +----------v----------+
                                        |  Azure Data Factory |
                                        | (ETL Pipelines)     |
                                        +----------+----------+
                                                   |
                                        +----------v----------+
                                        |   Azure Databricks  |
                                        |  (PySpark, Delta)   |
                                        +----------+----------+
                                                   |
                                        +----------v----------+
                                        |   Power BI Reports  |
                                        +---------------------+
```

---

## ğŸ“Š **Data Sources**  
1. **Electronic Medical Records (EMR) Data** - Stored in **Azure SQL DB**  
   - ğŸ”¹ Patients, Providers, Departments, Encounters, Transactions  
2. **Claims Data** - Provided by insurance companies in **CSV format** stored in **Azure Data Lake (ADLS Gen2)**  
   - ğŸ”¹ Includes claim IDs, amounts, statuses, payer info  
3. **Public APIs**  
   - ğŸ”— **NPI Data**: [NPI Registry](https://npiregistry.cms.hhs.gov/)  
   - ğŸ”— **ICD Codes**: [CDC ICD Codes](https://www.cdc.gov/nchs/icd/index.htm)  

---

## âš¡ **Key Features & Concepts Implemented**  

### âœ… **Data Ingestion & Integration**  
- ğŸ”„ Built **dynamic, parameterized ETL pipelines** using **Azure Data Factory**  
- ğŸŒ Fetched real-time data from **NPI & ICD code APIs**  
- ğŸ“ Ingested claims data from **Azure Data Lake** into **Azure Databricks**  

### ğŸ— **Data Transformation & Processing**  
- ğŸ› **Implemented Medallion Architecture** (**Bronze, Silver, Gold layers**) for structured data flow  
- ğŸ”¥ **PySpark transformations** in **Databricks** for scalable big data processing  
- ğŸ· **SCD Type 2 implementation** to manage historical patient and provider data  

### ğŸ›¡ **Data Governance & Optimization**  
- ğŸ” **Delta Lake** for ACID transactions ensuring data consistency  
- ğŸ•’ **Time Travel & Data Versioning** for audit and recovery  
- ğŸ“¦ **Parquet storage optimization** and **partitioning strategies**  

### ğŸ“ˆ **Analytics & Reporting**  
- ğŸ“Š Generated **fact and dimension tables** for KPI calculations  
- ğŸ¥ Enabled insights for **accounts receivable aging**, **collection efficiency**, and **claim denial trends**  
- ğŸŒŸ Connected final data models to **Power BI** for real-time dashboards  

---

## ğŸ› ï¸ **Tech Stack & Tools Used**  
- â˜ï¸ **Cloud:** Microsoft Azure (ADLS Gen2, Azure SQL DB, Azure Data Factory, Azure Databricks)  
- ğŸ”¥ **Big Data:** PySpark, Delta Lake  
- ğŸ›¢ **Storage:** Parquet, Delta  
- ğŸ“ˆ **Visualization:** Power BI  
- ğŸŒ **APIs:** NPI Registry API, ICD Codes API  

---

## ğŸ”§ **How to Run the Project**  
1. **Clone the Repository:**
   ```bash
   git clone https://github.com/tkaushik015/Healthcare-Data-Governance-and-ETL-Optimization-Using-Azure.git
   ```
2. **Azure Setup:**
   - Provision **Azure Data Factory**, **Azure SQL DB**, and **Azure Databricks** resources.
   - Upload claims data to **ADLS Gen2**.
3. **Pipeline Configuration:**
   - Configure **Data Factory pipelines** with proper linked services.
4. **Transformation Execution:**
   - Run **PySpark notebooks** in Databricks for data transformation.
5. **Reporting Setup:**
   - Connect **Power BI** to transformed datasets for visualization.

---

## ğŸ’¡ **Key Learnings & Challenges Faced**  
- ğŸ› Handling **schema mismatches** across multiple hospitals.
- âš¡ Ensuring **incremental data loads** without duplicates.
- ğŸ”„ Achieving **real-time updates** with minimal latency.

---

## ğŸ”¥ **Potential Future Improvements**  
- ğŸš€ Implement **Change Data Capture (CDC)** for real-time ingestion.
- ğŸ **Integrate streaming pipelines** using **Azure Event Hubs**.
- ğŸ”’ Enhance **security** with **Azure Key Vault**.
- ğŸ•’ Deploy **CI/CD pipelines** for production deployment.

---

## ğŸ§ª **Test Cases & Validation Strategies**  
- âœ… **Data validation checks** at each ETL stage.
- ğŸ”„ **Unit testing** for PySpark transformations.
- ğŸ“Š **Data quality checks** (null handling, duplicates removal).

---

## ğŸŒŸ **Project Impact & Use Cases**  
- ğŸ¥ **Faster revenue cycles** for healthcare providers.
- ğŸ“ˆ Enhanced **compliance tracking** for regulatory requirements.
- ğŸ’¡ **Improved financial decision-making** with actionable insights.

---

## ğŸ“š **References & Documentation**  
- ğŸ”— [Azure Data Factory Documentation](https://learn.microsoft.com/en-us/azure/data-factory/introduction)
- ğŸ”— [Delta Lake Documentation](https://docs.delta.io/latest/index.html)
- ğŸ”— [Azure Databricks Documentation](https://learn.microsoft.com/en-us/azure/databricks/)

---

âœ¨ **This comprehensive README covers architecture, data processing strategies, key insights, and deployment considerations, making it recruiter-friendly and technically robust.** ğŸš€ğŸ’¡

